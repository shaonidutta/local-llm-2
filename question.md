I want you to think in depth regarding each point and create a detailed PRD 

Build an AI App Using ONLY Local Inference”
Objective:

Build a simple app (choose 1):

✍️ AI Writer (e.g., blog intro / tweet / story from topic)
🗣️ Rephraser (e.g., rewrite like a CEO, teenager, comedian)
📚 Explainer (e.g., explain concept in simple words)
🔍 Custom Search (query over static docs using local embedding + RAG) ← if they’re ready
Constraints:

No OpenAI or Anthropic APIs
All logic must use a local LLM running on their machine
Requirements:

Prompt input box
Model output display
Temperature setting (optional)
Loading UI
Output logging (local file or frontend)
🔖 Submission Guidelines:
Submit a public GitHub repository link

A README that includes:

App type selected
How to run the app locally
Model used (llama3) 
Setup instructions