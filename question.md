I want you to think in depth regarding each point and create a detailed PRD 

Build an AI App Using ONLY Local Inferenceâ€
Objective:

Build a simple app (choose 1):

âœï¸ AI Writer (e.g., blog intro / tweet / story from topic)
ğŸ—£ï¸ Rephraser (e.g., rewrite like a CEO, teenager, comedian)
ğŸ“š Explainer (e.g., explain concept in simple words)
ğŸ” Custom Search (query over static docs using local embedding + RAG) â† if theyâ€™re ready
Constraints:

No OpenAI or Anthropic APIs
All logic must use a local LLM running on their machine
Requirements:

Prompt input box
Model output display
Temperature setting (optional)
Loading UI
Output logging (local file or frontend)
ğŸ”– Submission Guidelines:
Submit a public GitHub repository link

A README that includes:

App type selected
How to run the app locally
Model used (llama3) 
Setup instructions